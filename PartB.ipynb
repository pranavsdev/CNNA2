{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PartB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbyCIGpGcu17",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9b0e62c9-a01c-4cca-bbe0-80aac2b24d5e"
      },
      "source": [
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL3b_36vc0nw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2450d9f9-9f64-4d36-dc25-c6a31c8ff3c5"
      },
      "source": [
        "!unzip \"/content/gdrive/My Drive/Colab Notebooks/data.h5.zip\" -d \"./\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/My Drive/Colab Notebooks/data.h5.zip\n",
            "  inflating: ./data1.h5              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCB-lkg9dPlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb2ae9fc-31b9-46dd-93bd-4f7fc42d2bf1"
      },
      "source": [
        "\"\"\"\n",
        "Author: Pranav Srivastava\n",
        "file: PartB.py\n",
        "This file has implementation of PartB (i) and (ii)\n",
        "\n",
        "execution: functions with 10 variants of feature  extraction and 4 variants of fine tuning are available at the bottom of this file\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "def loadDataH5():\n",
        "    with h5py.File('data1.h5','r') as hf:\n",
        "        trainX = np.array(hf.get('trainX'))\n",
        "        trainY = np.array(hf.get('trainY'))\n",
        "        valX = np.array(hf.get('valX'))\n",
        "        valY = np.array(hf.get('valY'))\n",
        "        print (trainX.shape,trainY.shape)\n",
        "        print (valX.shape,valY.shape)\n",
        "    return trainX, trainY, valX, valY\n",
        "\n",
        "trainX, trainY, testX, testY = loadDataH5()\n",
        "\n",
        "def save_accuracy_score(filename, r, labels):\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/accuracy_results/\"\n",
        "    filename = path+filename+\".txt\"\n",
        "\n",
        "    accuracy_score = metrics.accuracy_score(r, labels)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(str(accuracy_score))\n",
        "        f.close()\n",
        "\n",
        "def save_model_summary(filename, model):\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/model_summary/\"\n",
        "    filename = path+filename+\".txt\"\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        with redirect_stdout(f):\n",
        "            model.summary()\n",
        "            f.close\n",
        "\n",
        "def plotAccLoss(H, NUM_EPOCHS):\n",
        "\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
        "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "    plt.title(\"Training Loss and Accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss/Accuracy\")\n",
        "    plt.legend(['train_loss', 'val_loss', 'train_acc', 'val_acc'], loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "def featureExtractionTransferLearning_variant1():\n",
        "\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    VGG16+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (vggModel.summary())\n",
        "    save_model_summary(\"FE_TL_VGG16_LogisticRegression\", vggModel)\n",
        "\n",
        "    featuresTrain = vggModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = vggModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(featuresTrain, trainY)\n",
        "\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"results are --->\")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_VGG16_LogisticRegression\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant2():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    VGG16+RandomForestClassifier_500\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (vggModel.summary())\n",
        "    save_model_summary(\"FE_TL_VGG16_RandomForestClassifier_500\", vggModel)\n",
        "\n",
        "    featuresTrain = vggModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = vggModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "    # 500 estimators (trees)\n",
        "    model = RandomForestClassifier(n_estimators=500)\n",
        "\n",
        "    model.fit(featuresTrain, trainY)\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"results are --->\")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_VGG16_RandomForestClassifier_200\", results, testY)\n",
        "\n",
        "def featureExtractionTransferLearning_variant3():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    InceptionV3+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    initialModel = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (initialModel.summary())\n",
        "    save_model_summary(\"FE_TL_Inception_v3_LogisticRegression\", initialModel)\n",
        "\n",
        "    featuresTrain = initialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = initialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    model.fit(featuresTrain, trainY)\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"accuracy: \")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_Inception_v3_LogisticRegression\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant4():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    InceptionV3+RandomForestClassifier_200\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    initialModel = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (initialModel.summary())\n",
        "    save_model_summary(\"FE_TL_Inception_v3_RandomForestClassifier_200\", initialModel)\n",
        "\n",
        "    featuresTrain = initialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = initialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "    # 500 estimators (trees)\n",
        "    model = RandomForestClassifier(n_estimators=200)\n",
        "\n",
        "    model.fit(featuresTrain, trainY)\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"accuracy: \")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_Inception_v3_RandomForestClassifier_200\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant5():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    ResNet152V2+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    initialModel = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (initialModel.summary())\n",
        "    save_model_summary(\"FE_TL_ResNet152V2_LogisticRegression\", initialModel)\n",
        "\n",
        "    featuresTrain = initialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = initialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    model.fit(featuresTrain, trainY)\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"accuracy: \")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_ResNet152V2_LogisticRegression\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant6():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    ResNet152V2+RandomForestClassifier_200\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "    initialModel = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    print (initialModel.summary())\n",
        "    save_model_summary(\"FE_TL_ResNet152V2_RandomForestClassifier_200\", initialModel)\n",
        "\n",
        "    featuresTrain = initialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = initialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "    # 500 estimators (trees)\n",
        "    model = RandomForestClassifier(n_estimators=200)\n",
        "\n",
        "    model.fit(featuresTrain, trainY)\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"accuracy: \")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_ResNet152V2_RandomForestClassifier_200\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant7():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    VGG16(till block4_conv2)+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Create a New Model Using a Portion of an Original Model\"\"\"\n",
        "\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    #cut the original network at block4_conv2\n",
        "    portionOfVGG16= tf.keras.Model(inputs=vggModel.input, outputs=vggModel.get_layer('block4_conv2').output)\n",
        "\n",
        "    print (portionOfVGG16.summary())\n",
        "    save_model_summary(\"FE_TL_portionOfVGG16_LogisticRegression\", portionOfVGG16)\n",
        "\n",
        "    featuresTrain = portionOfVGG16.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = portionOfVGG16.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(featuresTrain, trainY)\n",
        "\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"results are --->\")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_portionOfVGG16_LogisticRegression\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant8():\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    InceptionV3(till conv2d_50)+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Create a New Model Using a Portion of an Original Model\"\"\"\n",
        "\n",
        "    initialModel = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    #cut the original network at conv2d_50\n",
        "    portionOfInitialModel= tf.keras.Model(inputs=initialModel.input, outputs=initialModel.get_layer('conv2d_50').output)\n",
        "\n",
        "    print (portionOfInitialModel.summary())\n",
        "    save_model_summary(\"FE_TL_portionOfInceptionV3_LogisticRegression\", portionOfInitialModel)\n",
        "\n",
        "    featuresTrain = portionOfInitialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = portionOfInitialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(featuresTrain, trainY)\n",
        "\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"results are --->\")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_portionOfInceptionV3_LogisticRegression\", results, testY)\n",
        "\n",
        "def featureExtractionTransferLearning_variant9():\n",
        "\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    ResNet152V2(till conv4_block23_2_conv)+LogisticRegression\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Create a New Model Using a Portion of an Original Model\"\"\"\n",
        "\n",
        "    initialModel = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    #cut the original network at conv4_block23_2_conv\n",
        "    portionOfInitialModel= tf.keras.Model(inputs=initialModel.input, outputs=initialModel.get_layer('conv4_block23_2_conv').output)\n",
        "\n",
        "    print (portionOfInitialModel.summary())\n",
        "    save_model_summary(\"FE_TL_portionOfResNet152V2_LogisticRegression\", portionOfInitialModel)\n",
        "\n",
        "    featuresTrain = portionOfInitialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = portionOfInitialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(featuresTrain, trainY)\n",
        "\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"results are --->\")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    save_accuracy_score(\"FE_TL_portionOfResNet152V2_LogisticRegression\", results, testY)\n",
        "\n",
        "\n",
        "def featureExtractionTransferLearning_variant10():\n",
        "\n",
        "    \"\"\"\n",
        "    This is the variant of feature extraction\n",
        "    InceptionV3(till conv2d_50)+RandomForestClassifier_500\n",
        "    Implementaion for PartB (i)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Create a New Model Using a Portion of an Original Model\"\"\"\n",
        "\n",
        "    initialModel = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    #cut the original network at conv2d_50\n",
        "    portionOfInitialModel= tf.keras.Model(inputs=initialModel.input, outputs=initialModel.get_layer('conv2d_50').output)\n",
        "\n",
        "    print (portionOfInitialModel.summary())\n",
        "    save_model_summary(\"FE_TL_portionOfInceptionV3_RandomForestClassifier_500\", portionOfInitialModel)\n",
        "\n",
        "    featuresTrain = portionOfInitialModel.predict(trainX)\n",
        "    featuresTrain = featuresTrain.reshape(featuresTrain.shape[0], -1)\n",
        "\n",
        "    featuresVal = portionOfInitialModel.predict(testX)\n",
        "    featuresVal = featuresVal.reshape(featuresVal.shape[0], -1)\n",
        "\n",
        "\n",
        "    model = RandomForestClassifier(n_estimators=500)\n",
        "    model.fit(featuresTrain, trainY)\n",
        "\n",
        "    results = model.predict(featuresVal)\n",
        "    print(\"accuracy: \")\n",
        "    print (metrics.accuracy_score(results, testY))\n",
        "    \n",
        "\n",
        "    save_accuracy_score(\"FE_TL_portionOfInceptionV3_RandomForestClassifier_500\", results, testY)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fineTuning_Variant1():\n",
        "\n",
        "    \"\"\"\n",
        "    This is the variant of fine tuning\n",
        "    Pretrained network VGG16 is used in Phase A by removing fully convoluted layer\n",
        "    In Phase B, block4_conv1 onwards the layers are unfrozen\n",
        "    Implementaion for PartB (ii)\n",
        "    \"\"\"\n",
        "\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    vggModel.trainable = False\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(vggModel)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))  \n",
        "    model.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "\n",
        "    print (\"\\n Phase A - Training Fully Connected Layers\\n\")\n",
        "    \n",
        "    print(\"Compiling model...\")\n",
        "    opt = keras.optimizers.SGD(lr=0.01)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "    \n",
        "    #stop training when val_loss will not improve\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "    history=model.fit(trainX, trainY, epochs=NUM_EPOCHS, callbacks=[overfitCallback], batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, len(history.history['val_loss']))\n",
        "\n",
        "    print (\"\\n Phase B  - Fine Tune Fully Connected Layer and Selected Convolutional Layers \\n\")\n",
        "    vggModel.trainable = True\n",
        "    trainableFlag = False\n",
        "    \n",
        "    for layer in vggModel.layers:\n",
        "        if layer.name == 'block4_conv1':\n",
        "            trainableFlag = True\n",
        "        layer.trainable = trainableFlag\n",
        "    vggModel.summary()\n",
        "\n",
        "    for layer in vggModel.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    for layer in vggModel.layers:\n",
        "        sp= '  '[len(layer.name):]\n",
        "        print(\"sp--->\",layer.name,sp,layer.trainable)\n",
        "    #print(\"model summary--->\", model.summary())\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(lr=1e-5),metrics=[\"accuracy\"])\n",
        "    history =model.fit(trainX, trainY, epochs=NUM_EPOCHS, batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, NUM_EPOCHS)\n",
        "\n",
        "def fineTuning_Variant2():\n",
        "    \"\"\"\n",
        "    Implementaion for PartB (ii)\n",
        "    Details of this variant  \n",
        "    Phase A: portion of InceptionV3 (till conv2d_39)\n",
        "    Phase B: unfreeze the convolutional layers block with a very low learning rate\n",
        "    \"\"\"\n",
        "    initialModel = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "    \n",
        "    #cut the original network at conv2d_39\n",
        "    portionOfInitialModel= tf.keras.Model(inputs=initialModel.input, outputs=initialModel.get_layer('conv2d_39').output)\n",
        "\n",
        "\n",
        "    portionOfInitialModel.trainable = False\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(portionOfInitialModel)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(256, activation='relu'))  \n",
        "    model.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "\n",
        "    print (\"\\n Phase A - Training Fully Connected Layers\\n\")\n",
        "    \n",
        "    print(\"Compiling model...\")\n",
        "    opt = keras.optimizers.SGD(lr=0.01)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "    \n",
        "    #stop training when val_loss will not improve\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "    history=model.fit(trainX, trainY, epochs=NUM_EPOCHS, callbacks=[overfitCallback], batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, len(history.history['val_loss']))\n",
        "\n",
        "    print (\"\\n Phase B  - Fine Tune Fully Connected Layer and Selected Convolutional Layers \\n\")\n",
        "    portionOfInitialModel.trainable = True\n",
        "    trainableFlag = False\n",
        "    \n",
        "    for layer in portionOfInitialModel.layers:\n",
        "\n",
        "        #unfreeze all the layers from block4_conv1 onwards\n",
        "        if layer.name == 'conv2d_30':\n",
        "            trainableFlag = True\n",
        "        layer.trainable = trainableFlag\n",
        "    portionOfInitialModel.summary()\n",
        "\n",
        "    for layer in portionOfInitialModel.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    for layer in portionOfInitialModel.layers:\n",
        "        sp= '  '[len(layer.name):]\n",
        "        print(\"sp--->\",layer.name,sp,layer.trainable)\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(lr=1e-5),metrics=[\"accuracy\"])\n",
        "    history =model.fit(trainX, trainY, epochs=NUM_EPOCHS, batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, NUM_EPOCHS)\n",
        "\n",
        "\n",
        "\n",
        "def fineTuning_Variant3():\n",
        "\n",
        "    \"\"\"\n",
        "    Implementaion for PartB (ii)\n",
        "    This is the variant of fine tuning\n",
        "    Pretrained network VGG16 is used in Phase A by removing fully convoluted layer\n",
        "    In Phase B, block4_conv1 onwards the layers are unfrozen\n",
        "    \"\"\"\n",
        "\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    vggModel.trainable = False\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(vggModel)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(500, activation='relu')) \n",
        "    model.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "\n",
        "    print (\"\\n Phase A - Training Fully Connected Layers\\n\")\n",
        "    \n",
        "    print(\"Compiling model...\")\n",
        "    opt = keras.optimizers.SGD(lr=0.01)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "    \n",
        "    #stop training when val_loss does not improve\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "    history=model.fit(trainX, trainY, epochs=NUM_EPOCHS, callbacks=[overfitCallback], batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, len(history.history['val_loss']))\n",
        "\n",
        "    print (\"\\n Phase B  - Fine Tune Fully Connected Layer and Selected Convolutional Layers \\n\")\n",
        "    vggModel.trainable = True\n",
        "    trainableFlag = False\n",
        "    \n",
        "    for layer in vggModel.layers:\n",
        "        if layer.name == 'block4_conv1':\n",
        "            trainableFlag = True\n",
        "        layer.trainable = trainableFlag\n",
        "    vggModel.summary()\n",
        "\n",
        "    for layer in vggModel.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "\n",
        "    for layer in vggModel.layers:\n",
        "        sp= '  '[len(layer.name):]\n",
        "        print(\"sp--->\",layer.name,sp,layer.trainable)\n",
        "    #print(\"model summary--->\", model.summary())\n",
        "\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(lr=1e-5),metrics=[\"accuracy\"])\n",
        "    history =model.fit(trainX, trainY, epochs=NUM_EPOCHS, batch_size=32, validation_data=(testX, testY))\n",
        "\n",
        "    plotAccLoss(history, NUM_EPOCHS)\n",
        "\n",
        "\n",
        "def fineTuning_Variant4():\n",
        "\n",
        "    \"\"\"\n",
        "    Implementaion for PartB (ii)\n",
        "    This is the variant of fine tuning\n",
        "    Pretrained network VGG16 is used in Phase A by removing fully convoluted layer\n",
        "    In Phase B, block4_conv1 onwards the layers are unfrozen\n",
        "    In Phase A, no dropout are applied in the newly created fully connected layer.\n",
        "    \"\"\"\n",
        "\n",
        "    vggModel = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "    vggModel.trainable = False\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(vggModel)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(500, activation='relu')) \n",
        "    model.add(tf.keras.layers.Dense(17, activation='softmax'))\n",
        "    \n",
        "    print (\"\\n Phase A - Training Fully Connected Layers\\n\")\n",
        "    print(\"Compiling model...\")\n",
        "    opt = keras.optimizers.SGD(lr=0.01)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\n",
        "    #stop training when val_loss does not improve\n",
        "    overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "    history=model.fit(trainX, trainY, epochs=NUM_EPOCHS, callbacks=[overfitCallback], batch_size=32, validation_data=(testX, testY))\n",
        "    plotAccLoss(history, len(history.history['val_loss']))\n",
        "    print (\"\\n Phase B  - Fine Tune Fully Connected Layer and Selected Convolutional Layers \\n\")\n",
        "    vggModel.trainable = True\n",
        "    trainableFlag = False\n",
        "    for layer in vggModel.layers:\n",
        "        if layer.name == 'block4_conv1':\n",
        "            trainableFlag = True\n",
        "        layer.trainable = trainableFlag\n",
        "    vggModel.summary()\n",
        "    for layer in vggModel.layers[:-3]:\n",
        "        layer.trainable=False\n",
        "    for layer in vggModel.layers:\n",
        "        sp= '  '[len(layer.name):]\n",
        "        print(\"sp--->\",layer.name,sp,layer.trainable)\n",
        "    #print(\"model summary--->\", model.summary())\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=keras.optimizers.SGD(lr=1e-5),metrics=[\"accuracy\"])\n",
        "    history =model.fit(trainX, trainY, epochs=NUM_EPOCHS, batch_size=32, validation_data=(testX, testY))\n",
        "    plotAccLoss(history, NUM_EPOCHS)\n",
        "\n",
        "#featureExtractionTransferLearning_NN()\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#                        execution details\n",
        "###############################################################################\n",
        "\n",
        "\"\"\"\n",
        "please create folders 'model_summary' and 'accuracy_results' as the results will be stored there\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"please uncomment any of the fucntion to execute\"\"\"\n",
        "\n",
        "#featureExtractionTransferLearning_variant1()\n",
        "#featureExtractionTransferLearning_variant2()\n",
        "#featureExtractionTransferLearning_variant3()\n",
        "#featureExtractionTransferLearning_variant4()\n",
        "#featureExtractionTransferLearning_variant5()\n",
        "#featureExtractionTransferLearning_variant6()\n",
        "#featureExtractionTransferLearning_variant7()\n",
        "#featureExtractionTransferLearning_variant8()\n",
        "#featureExtractionTransferLearning_variant9()\n",
        "#featureExtractionTransferLearning_variant10()\n",
        "#featureExtractionTransferLearning_variant11()\n",
        "\n",
        "#fineTuning_Variant1()\n",
        "#fineTuning_Variant2()\n",
        "#fineTuning_Variant3()\n",
        "#fineTuning_Variant4()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1020, 128, 128, 3) (1020,)\n",
            "(340, 128, 128, 3) (340,)\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "results are --->\n",
            "0.8764705882352941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}